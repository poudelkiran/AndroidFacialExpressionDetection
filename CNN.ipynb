{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d378e7df9d164dc184c663e2abfda0eea4f4372"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\ndataset_path = '../input/fer2013.csv'\nimage_size=(48,48)\n \ndef load_fer2013():\n    data = pd.read_csv(dataset_path)\n    pixels = data['pixels'].tolist()\n    width, height = 48, 48\n    faces = []\n    for pixel_sequence in pixels:\n        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n        face = np.asarray(face).reshape(width, height)\n        face = cv2.resize(face.astype('uint8'),image_size)\n        faces.append(face.astype('float32'))\n    faces = np.asarray(faces)\n    faces = np.expand_dims(faces, -1)\n    emotions = pd.get_dummies(data['emotion']).as_matrix()\n    return faces, emotions\n \ndef preprocess_input(x, v2=True):\n    x = x.astype('float32')\n    x = x / 255.0\n    if v2:\n        x = x - 0.5\n        x = x * 2.0\n    return x\n \nfaces, emotions = load_fer2013()\nfaces = preprocess_input(faces)\nxtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Activation, Convolution2D, Dropout, Conv2D\nfrom keras.layers import AveragePooling2D, BatchNormalization\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.models import Sequential\nfrom keras.layers import Flatten\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import SeparableConv2D\nfrom keras import layers\nfrom keras.regularizers import l2\nimport pandas as pd\nimport cv2\nimport numpy as np\n \n# parameters\nbatch_size = 32\nnum_epochs = 3\ninput_shape = (48, 48, 1)\nverbose = 1\nnum_classes = 7\npatience = 50\nl2_regularization=0.01\n \n# data generator\ndata_generator = ImageDataGenerator(\n                        featurewise_center=False,\n                        featurewise_std_normalization=False,\n                        rotation_range=10,\n                        width_shift_range=0.1,\n                        height_shift_range=0.1,\n                        zoom_range=.1,\n                        horizontal_flip=True)\n \n# model parameters\nregularization = l2(l2_regularization)\n \n# base\nimg_input = Input(input_shape)\nx = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(img_input)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n \n# module 1\nresidual = Conv2D(16, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\nresidual = BatchNormalization()(residual)\nx = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\nx = layers.add([x, residual])\n \n# module 2\nresidual = Conv2D(32, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\nresidual = BatchNormalization()(residual)\nx = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\nx = layers.add([x, residual])\n \n# module 3\nresidual = Conv2D(64, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\nresidual = BatchNormalization()(residual)\nx = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\nx = layers.add([x, residual])\n \n# module 4\nresidual = Conv2D(128, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\nresidual = BatchNormalization()(residual)\nx = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\nx = layers.add([x, residual])\nx = Conv2D(num_classes, (3, 3), padding='same')(x)\nx = GlobalAveragePooling2D()(x)\noutput = Activation('softmax',name='predictions')(x)\n \nmodel = Model(img_input, output)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()\n \n# callbacks\nlog_file_path = '_emotion_training.log'\ncsv_logger = CSVLogger(log_file_path, append=False)\nearly_stop = EarlyStopping('val_loss', patience=patience)\nreduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n\nmodel_names = 'hdf5file.hdf5'\nmodel_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\ncallbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n \nmodel.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),\n                        steps_per_epoch=len(xtrain) / batch_size,\n                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n                        validation_data=(xtest,ytest))# This Python 3 environment comes with many helpful analytics libraries installed\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%file ex13.py\n#!/usr/bin/env python\n\"\"\"\nCopyright (c) 2018, by the Authors: Amir H. Abdi\nThis script is freely available under the MIT Public License.\nPlease see the License file in the root for details.\nThe following code snippet will convert the keras model files\nto the freezed .pb tensorflow weight file. The resultant TensorFlow model\nholds both the model architecture and its associated weights.\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import graph_io\nfrom pathlib import Path\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport keras\nfrom keras import backend as K\nfrom keras.models import model_from_json\n\nK.set_learning_phase(0)\n\ntf.app.flags.DEFINE_string('f', '', 'kernel')\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('input_model', None, 'path to input file')\nflags.DEFINE_string('input_model_json', None, 'Path to the input model '\n                                              'architecture in json format.')\nflags.DEFINE_string('output_model', None, 'path to output file with extension,eg. classifier.pb')\nflags.DEFINE_boolean('save_graph_def', False,\n                     'Whether to save the graphdef.pbtxt file which contains '\n                     'the graph definition in ASCII format.')\nflags.DEFINE_string('output_nodes_prefix', None,\n                    'If set, the output nodes will be renamed to '\n                    '`output_nodes_prefix`+i, where `i` will numerate the '\n                    'number of of output nodes of the network.')\nflags.DEFINE_boolean('quantize', False,\n                     'If set, the resultant TensorFlow graph weights will be '\n                     'converted from float into eight-bit equivalents. See '\n                     'documentation here: '\n                     'https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms')\nflags.DEFINE_boolean('channels_first', False,\n                     'Whether channels are the first dimension of a tensor. '\n                     'The default is TensorFlow behaviour where channels are '\n                     'the last dimension.')\nflags.DEFINE_boolean('output_meta_ckpt', False,\n                     'If set to True, exports the model as .meta, .index, and '\n                     '.data files, with a checkpoint file. These can be later '\n                     'loaded in TensorFlow to continue training.')\n\nflags.mark_flag_as_required('input_model')\nflags.mark_flag_as_required('output_model')\n\n\ndef load_model(input_model_path, input_json_path):\n    if not Path(input_model_path).exists():\n        raise FileNotFoundError(\n            'Model file `{}` does not exist.'.format(input_model_path))\n    try:\n        model = keras.models.load_model(input_model_path)\n        return model\n    except FileNotFoundError as err:\n        logging.error('Input mode file (%s) does not exist.', FLAGS.input_model)\n        raise err\n    except ValueError as wrong_file_err:\n        if input_json_path:\n            if not Path(input_json_path).exists():\n                raise FileNotFoundError(\n                    'Model description json file `{}` does not exist.'.format(\n                        input_json_path))\n            try:\n                model = model_from_json(open(str(input_json_path)).read())\n                model.load_weights(input_model_path)\n                return model\n            except Exception as err:\n                logging.error(\"Couldn't load model from json.\")\n                raise err\n        else:\n            logging.error(\n                'Input file specified only holds the weights, and not '\n                'the model definition. Save the model using '\n                'model.save(filename.h5) which will contain the network '\n                'architecture as well as its weights. If the model is '\n                'saved using model.save_weights(filename), the flag '\n                'input_model_json should also be set to the '\n                'architecture which is exported separately in a '\n                'json format. Check the keras documentation for more details '\n                '(https://keras.io/getting-started/faq/)')\n            raise wrong_file_err\n\n\ndef main(args):\n    # If output_model path is relative and in cwd, make it absolute from root\n    output_model = FLAGS.output_model\n    if str(Path(output_model).parent) == '.':\n        output_model = str((Path.cwd() / output_model))\n\n    output_fld = Path(output_model).parent\n    output_model_name = Path(output_model).name\n    output_model_stem = Path(output_model).stem\n    output_model_pbtxt_name = output_model_stem + '.pbtxt'\n\n    # Create output directory if it does not exist\n    Path(output_model).parent.mkdir(parents=True, exist_ok=True)\n\n    if FLAGS.channels_first:\n        K.set_image_data_format('channels_first')\n    else:\n        K.set_image_data_format('channels_last')\n\n    model = load_model(FLAGS.input_model, FLAGS.input_model_json)\n\n    # TODO(amirabdi): Support networks with multiple inputs\n    orig_output_node_names = [node.op.name for node in model.outputs]\n    if FLAGS.output_nodes_prefix:\n        num_output = len(orig_output_node_names)\n        pred = [None] * num_output\n        converted_output_node_names = [None] * num_output\n\n        # Create dummy tf nodes to rename output\n        for i in range(num_output):\n            converted_output_node_names[i] = '{}{}'.format(\n                FLAGS.output_nodes_prefix, i)\n            pred[i] = tf.identity(model.outputs[i],\n                                  name=converted_output_node_names[i])\n    else:\n        converted_output_node_names = orig_output_node_names\n    logging.info('Converted output node names are: %s',\n                 str(converted_output_node_names))\n\n    sess = K.get_session()\n    if FLAGS.output_meta_ckpt:\n        saver = tf.train.Saver()\n        saver.save(sess, str(output_fld / output_model_stem))\n\n    if FLAGS.save_graph_def:\n        tf.train.write_graph(sess.graph.as_graph_def(), str(output_fld),\n                             output_model_pbtxt_name, as_text=True)\n        logging.info('Saved the graph definition in ascii format at %s',\n                     str(Path(output_fld) / output_model_pbtxt_name))\n\n    if FLAGS.quantize:\n        from tensorflow.tools.graph_transforms import TransformGraph\n        transforms = [\"quantize_weights\", \"quantize_nodes\"]\n        transformed_graph_def = TransformGraph(sess.graph.as_graph_def(), [],\n                                               converted_output_node_names,\n                                               transforms)\n        constant_graph = graph_util.convert_variables_to_constants(\n            sess,\n            transformed_graph_def,\n            converted_output_node_names)\n    else:\n        constant_graph = graph_util.convert_variables_to_constants(\n            sess,\n            sess.graph.as_graph_def(),\n            converted_output_node_names)\n\n    graph_io.write_graph(constant_graph, str(output_fld), output_model_name,\n                         as_text=False)\n    logging.info('Saved the freezed graph at %s',\n                 str(Path(output_fld) / output_model_name))\n\n\nif __name__ == \"__main__\":\n    app.run(main)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2ea0f1a7f06931f0e5f5912141f8c7e33cd8c10",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "%%!\npython ex13.py  --save_graph_def True --input_model='hdf5file.hdf5' --output_model='frozen.pb'",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "['Using TensorFlow backend.',\n 'Traceback (most recent call last):',\n '  File \"ex13.py\", line 167, in <module>',\n '    app.run(main)',\n '  File \"/opt/conda/lib/python3.6/site-packages/absl/app.py\", line 300, in run',\n '    _run_main(main, args)',\n '  File \"/opt/conda/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main',\n '    sys.exit(main(argv))',\n '  File \"ex13.py\", line 113, in main',\n '    model = load_model(FLAGS.input_model, FLAGS.input_model_json)',\n '  File \"ex13.py\", line 60, in load_model',\n \"    'Model file `{}` does not exist.'.format(input_model_path))\",\n 'FileNotFoundError: Model file `../1.hdf5` does not exist.']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4fa2873d687cb791f1108e6c19dfb5068e02a03f"
      },
      "cell_type": "code",
      "source": "# From frozen pb to optimize pb\nimport tensorflow as tf\nfrom tensorflow.python.tools import optimize_for_inference_lib\ninput_node_names = ['input_1']\noutput_node_name = 'predictions/Softmax'\ninput_graph_def = tf.GraphDef()\nwith tf.gfile.Open('frozen.pb', \"rb\") as f:\n    input_graph_def.ParseFromString(f.read())\noutput_graph_def = optimize_for_inference_lib.optimize_for_inference(\n            input_graph_def, input_node_names, [output_node_name],\n            tf.float32.as_datatype_enum)\nwith tf.gfile.FastGFile('optimize.pb', \"wb\") as f:\n    f.write(output_graph_def.SerializeToString())",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}